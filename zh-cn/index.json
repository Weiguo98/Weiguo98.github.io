[{"content":"","date":"2024-08-14","externalUrl":null,"permalink":"/zh-cn/tags/ansible/","section":"Tags","summary":"","title":"Ansible","type":"tags"},{"content":"在使用Ansible进行环境配置时，处理错误是一个关键问题，尽管有ignore_errors和fails_when两个keyword支持，但他们只能应用在具体的task，不能应用在role/整个playbook上。本文将介绍如何在复杂设置中如何更好的处理错误，特别是对于应用ansible在测试的情况下。\n问题描述 # ignore_errors=true在Ansible角色中不起作用，只在会话中有效。需要在测试任务和后续任务中多次添加此选项，以确保如果测试或者仅仅是不能够获取日志，下一个测试仍然可以运行。那如何更好的处理这种情况避免滥用ignore_errors 以及如果某个测试的预处理任务失败了，该测试可以被跳过，而不是被执行？\n解决方案是，可以重构测试用例，使用block、rescue、always和clear_host_errors。 以下是一个可以工作的示例：\n- block: - name: Pre tasks command: /bin/false - name: Test tasks command: /bin/false register: result rescue: - name: Record error ansible.builtin.debug: var: result always: - name: Post tasks command: echo \u0026#34;post tasks\u0026#34; - name: Clear host errors meta: clear_host_errors 详细说明 # 1.使用ignore_errors 在Ansible中，如果一个任务失败，Ansible默认会停止在该主机上执行任务。但是，你可以使用ignore_errors来忽略错误并继续执行。例如：\n- name: Attempt to Execute a Command command: /some/nonexistent/command ignore_errors: true # This task will fail, but the playbook will continue. 2.使用block、rescue和always 通过将每个角色的任务分解为多个部分，并使用block、rescue和always关键字，可以实现更细粒度的错误处理。 从上面的例子可以看到如果pre task失败了，test task不会被执行，因为他们在一个block里面。ansible task的失败会trigger ansible执行rescue block， 在这里我们可以记录错误，例子中只是使用了debug，也可以写入到verdict/任何文件中。无论block中的task执行状态如何，都会执行post task，在post task中可以加入collect logs等任务，即使失败，在always中的任务仍然会被执行。最后通过clear_host_error 将error删除，这样ansible不会停止在该主机上执行任务，下一个test block仍然会被执行。\n这种结构可以构建多个test在一个playbook中，并且互不影响，能够从一定程度上减少同一时间需要启动的vm的数量。\n参考资料 # ansible error handling ","date":"2024-08-14","externalUrl":null,"permalink":"/zh-cn/blog/ansible-ignore-errors/ansible-ignore-errors/","section":"Blogs","summary":"在使用Ansible进行环境配置时，处理错误是一个关键问题，","title":"Ansible complex set up error handling","type":"blog"},{"content":"","date":"2024-08-14","externalUrl":null,"permalink":"/zh-cn/blog/","section":"Blogs","summary":"","title":"Blogs","type":"blog"},{"content":"","date":"2024-08-14","externalUrl":null,"permalink":"/zh-cn/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"2024-08-14","externalUrl":null,"permalink":"/zh-cn/","section":"Wei的博客","summary":"","title":"Wei的博客","type":"page"},{"content":"","date":"2023-05-25","externalUrl":null,"permalink":"/zh-cn/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes","type":"tags"},{"content":"markdownlint-disable MD010 当我们需要从 Helm Chart 向服务传递少量数据时，可以很容易地在 deployment.yaml 中使用 container.env 部分。这样我们只需在代码中使用 os.getEnv()就能获取我们想要的值。\n但是如果我们有大量的值需要传递，将它们全部添加到 Helm Chart 中会很混乱。这时，我们可以使用配置映射。个人更喜欢使用 JSON 格式，因为它易于解组为结构体，并且可以轻松访问所有值。\n以下是通过配置映射传递数据的具体步骤：\n使用 JSON 格式的数据设置一个配置映射。\napiVersion: \u0026#34;v1\u0026#34; kind: \u0026#34;ConfigMap\u0026#34; metadata: name: json-configmap data: config.json: | { \u0026#34;key1\u0026#34;: \u0026#34;value1\u0026#34;, \u0026#34;key2\u0026#34;: \u0026#34;value2\u0026#34; } 在 Pod 中创建一个卷并将其挂载到我们想要的容器中。\napiVersion: \u0026#34;apps/v1\u0026#34; kind: \u0026#34;Deployment\u0026#34; metadata: name: \u0026#34;test-service\u0026#34; spec: - template: metadata: volumes: - name: \u0026#34;json-config-volume\u0026#34; configMap: // 确保名称与 configmap metadata.name 中的名称相同 name: \u0026#34;json-configmap\u0026#34; containers: - name: \u0026#34;container-first\u0026#34; volumeMounts: - name: \u0026#34;config-volume\u0026#34; mountPath: \u0026#34;/etc/config\u0026#34; // 不是必须的，你可以在代码中硬编码，这里放置只是为了确保 // 如果有一天我们更改了挂载路径，确保也更改了环境变量。 env: - name: \u0026#34;CONFIG_FILE\u0026#34; value: \u0026#34;/etc/config/config.json\u0026#34; 在代码中，我们可以通过 os.getEnv() 轻松访问 JSON 文件并对其进行解组。\n// 基于 JSON 格式 type JsonConfig struct{ key1 string key2 string } func getJsonConfig() error{ filePath, ok := os.LookupEnv(\u0026#34;CONFIG_FILE\u0026#34;) if !ok { return errors.New(\u0026#34;failed to get file path\u0026#34;) } config, err := os.Open(filePath) if err != nil { return errors.Wrap(err, \u0026#34;failed to open file\u0026#34;) } defer config.Close() byteValue, err := io.ReadAll(config) if err != nil { return errors.Wrap(err, \u0026#34;failed to read file\u0026#34;) } jsonConfig := \u0026amp;JsonConfig{} if err = json.Unmarshal(byteValue, jsonConfig); err != nil { return errors.Wrap(err, \u0026#34;failed to unmarshal file\u0026#34;) } return nil } ","date":"2023-05-25","externalUrl":null,"permalink":"/zh-cn/blog/pass-data-in-json/pass-data-in-json/","section":"Blogs","summary":"markdownlint-disable MD010 当我们需要从 Helm Chart 向服务传递少量数据时，可以很容易地在 deployment.yaml 中","title":"如何在配置映射中以 JSON 格式传递数据到微服务","type":"blog"},{"content":" Readiness and Liveness probe Configure Liveness, Readiness and Startup Probes | Kubernetes The kubelet uses liveness probes to know when to restart a container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a container in such a state can help to make the application more available despite bugs. The kubelet uses readiness probes to know when a container is ready to start accepting traffic. A Pod is considered ready when all of its containers are ready. One use of this signal is to control which Pods are used as backends for Services. When a Pod is not ready, it is removed from Service load balancers. The readiness check will continuously run in the pod lifecycle. As long as Liveness Probe passed, the pod status changed to running. If you want to connect to a pod when it is ready for traffic, it is better to check the ready keywords.\nPod lifecycle Pod Lifecycle | Kubernetes Value Description Pending The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to be scheduled as well as the time spent downloading container images over the network. Running The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting. Succeeded All containers in the Pod have terminated in success, and will not be restarted. Failed All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system. Unknown For some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running. When a Pod is being deleted, it is shown as Terminating by some kubectl commands. This Terminating status is not one of the Pod phases. A Pod is granted a term to terminate gracefully, which defaults to 30 seconds. You can use the flag --force to terminate a Pod by force. ","date":"25 January 2023","externalUrl":null,"permalink":"/blog/kubernetes-pods-lifecycle/","section":"Blogs","summary":"Readiness and Liveness probe Configure Liveness, Readiness and Startup Probes | Kubernetes The kubelet uses liveness probes to know when to restart a container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress.","title":"Kubernetes probes and pod lifecycle","type":"blog"},{"content":"","externalUrl":null,"permalink":"/zh-cn/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/zh-cn/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/zh-cn/series/","section":"Series","summary":"","title":"Series","type":"series"}]